{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T08:20:54.843071Z",
     "iopub.status.busy": "2023-04-13T08:20:54.842424Z",
     "iopub.status.idle": "2023-04-13T08:20:56.333246Z",
     "shell.execute_reply": "2023-04-13T08:20:56.332446Z",
     "shell.execute_reply.started": "2023-04-13T08:20:54.843045Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "DATA_FOLDER = 'data'\n",
    "DATA_TEST_FILE = 'test.csv'\n",
    "DATA_TRAIN_FILE = 'train.csv'\n",
    "DATA_SAMPLE_SUBMISSION_FILE = 'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "## Dataset Description\n",
    "What files do I need?\n",
    "You'll need `train.csv`, `test.csv` and `sample_submission.csv`.\n",
    "\n",
    "## What should I expect the data format to be?\n",
    "Each sample in the train and test set has the following information:\n",
    "\n",
    "The `text` of a tweet\n",
    "A `keyword` from that tweet (although this may be blank!)\n",
    "The `location` the tweet was sent from (may also be blank)\n",
    "## What am I predicting?\n",
    "You are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n",
    "\n",
    "## Files\n",
    "- train.csv - the training set\n",
    "- test.csv - the test set\n",
    "- sample_submission.csv - a sample submission file in the correct format\n",
    "  \n",
    "## Columns\n",
    "- `id` - a unique identifier for each tweet\n",
    "- `text` - the text of the tweet\n",
    "- `location` - the location the tweet was sent from (may be blank)\n",
    "- `keyword` - a particular keyword from the tweet (may be blank)\n",
    "- `target` - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is for loading and exploring the training and testing datasets for a research project related to disaster response. It uses the pandas library to read in csv files located in the specified `DATA_FOLDER` directory, and fill any missing values with an empty string.\n",
    "\n",
    "The `train_df` and `test_df` dataframes are created by reading in the training and testing datasets respectively. Each dataframe has columns named 'id', 'keyword', 'location', 'text', and 'target', where 'target' is the binary classification label indicating whether a tweet is about a real disaster or not.\n",
    "\n",
    "The `sample_submission_df` dataframe is also loaded from a csv file located in the `DATA_FOLDER`. This is likely the format that the submission file should follow for the competition or project that this code is part of.\n",
    "\n",
    "The `train_df.head(5)` line is used to display the first 5 rows of the `train_df` dataframe.\n",
    "\n",
    "Lastly, there are four `print` statements which output the number of unique values in the 'keyword' and 'location' columns for the training and testing datasets respectively. This gives an idea of how many different keywords or locations are present in the datasets, which could be useful for understanding the data or for feature engineering later on in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T08:20:56.334915Z",
     "iopub.status.busy": "2023-04-13T08:20:56.334586Z",
     "iopub.status.idle": "2023-04-13T08:20:56.400374Z",
     "shell.execute_reply": "2023-04-13T08:20:56.399715Z",
     "shell.execute_reply.started": "2023-04-13T08:20:56.334893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1                   Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4                              Forest fire near La Ronge Sask. Canada   \n",
       "2   5                   All residents asked to 'shelter in place' are ...   \n",
       "3   6                   13,000 people receive #wildfires evacuation or...   \n",
       "4   7                   Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_FOLDER, DATA_TRAIN_FILE))\n",
    "train_df = train_df.fillna('')\n",
    "test_df = pd.read_csv(os.path.join(DATA_FOLDER, DATA_TEST_FILE))\n",
    "test_df = test_df.fillna('')\n",
    "\n",
    "sample_submission_df = pd.read_csv(os.path.join(DATA_FOLDER, DATA_SAMPLE_SUBMISSION_FILE))\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T08:20:56.401628Z",
     "iopub.status.busy": "2023-04-13T08:20:56.401421Z",
     "iopub.status.idle": "2023-04-13T08:20:56.407472Z",
     "shell.execute_reply": "2023-04-13T08:20:56.406806Z",
     "shell.execute_reply.started": "2023-04-13T08:20:56.401609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Number of unique values in keyword = 222\n",
      "Training - Number of unique values in location = 3342\n",
      "\n",
      "Testing - Number of unique values in keyword = 222\n",
      "Testing - Number of unique values in location = 1603\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training - Number of unique values in keyword = {train_df['keyword'].nunique()}\")\n",
    "print(f\"Training - Number of unique values in location = {train_df['location'].nunique()}\\n\")\n",
    "\n",
    "print(f\"Testing - Number of unique values in keyword = {test_df['keyword'].nunique()}\")\n",
    "print(f\"Testing - Number of unique values in location = {test_df['location'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T08:20:56.409295Z",
     "iopub.status.busy": "2023-04-13T08:20:56.409096Z",
     "iopub.status.idle": "2023-04-13T08:20:56.422026Z",
     "shell.execute_reply": "2023-04-13T08:20:56.421287Z",
     "shell.execute_reply.started": "2023-04-13T08:20:56.409276Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(remove_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a PyTorch neural network class called `BertClassifier`, which is used to perform binary classification on text data using a pre-trained BERT (Bidirectional Encoder Representations from Transformers) model.\n",
    "\n",
    "The `__init__` method initializes the neural network architecture. The `BertModel.from_pretrained('bert-base-cased')` line loads a pre-trained BERT model from the Hugging Face Transformers library. The `dropout` layer randomly drops some of the outputs during training to prevent overfitting. The `fc1` and `fc2` fully connected layers are used to transform the output from the BERT model into a single output value that represents the probability of the input text being about a real disaster.\n",
    "\n",
    "The `forward` method defines how the input data is processed through the neural network. The `input_ids` and `attention_mask` inputs are passed through the pre-trained BERT model to obtain a `pooled_output`, which represents the output of the last hidden layer of the BERT model for the entire input sequence. The `pooled_output` is then passed through a ReLU activation function and the fully connected layers to obtain the final `logits` output, which is a single value representing the probability of the input text being about a real disaster.\n",
    "\n",
    "The `tokenizer` line initializes a BERT tokenizer from the Hugging Face Transformers library, which is used to convert text data into input features that can be fed into the `BertClassifier model`. The `BertTokenizerFast.from_pretrained('bert-base-cased')` line loads a pre-trained BERT tokenizer that has been optimized for fast tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T08:20:56.423499Z",
     "iopub.status.busy": "2023-04-13T08:20:56.423289Z",
     "iopub.status.idle": "2023-04-13T08:20:57.048460Z",
     "shell.execute_reply": "2023-04-13T08:20:57.047789Z",
     "shell.execute_reply.started": "2023-04-13T08:20:56.423479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a51d5027e28483394644408ed69b50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff89d48e3bcc4233a5cd68a53ec95e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73e61a80d254451b563e2adb7ebb553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95065aca8ffd461e8969d4de5856bb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        hidden = nn.functional.relu(self.fc1(pooled_output))\n",
    "        logits = self.fc2(hidden)\n",
    "        return logits\n",
    "    \n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "This code defines a function `train_and_evaluate` that trains and evaluates a given BERT-based binary text classifier model using the provided training and validation datasets. The function takes three inputs:\n",
    "\n",
    "- `model`: an instance of the `BertClassifier` class defined earlier, representing the BERT-based text classifier.\n",
    "- `train_data`: a pandas dataframe containing the training data, with columns 'text' and 'target'.\n",
    "- `val_data`: a pandas dataframe containing the validation data, with columns 'text' and 'target'.\n",
    "\n",
    "The function performs the following steps:\n",
    "\n",
    "1. Extract the labels and texts from the training and validation datasets.\n",
    "2. Tokenize the texts using the BERT tokenizer, pad the sequences to a maximum length of 32, and convert the resulting tokenized texts to PyTorch tensors.\n",
    "3. Create PyTorch TensorDataset objects for the tokenized texts and labels, and then create PyTorch DataLoader objects for the training and validation datasets.\n",
    "4. Set the device for the PyTorch tensors to either 'cuda' or 'cpu' depending on availability of a GPU.\n",
    "5. Define the loss function and optimizer for training the BERT model.\n",
    "6. Train the model for a specified number of epochs (EPOCHS), using the training dataset and validation dataset in each epoch.\n",
    "7. For each epoch, compute and print the training and validation loss and accuracy metrics.\n",
    "9. Return the trained model.\n",
    "\n",
    "During training, the model is evaluated on the validation dataset at the end of each epoch to prevent overfitting. The `train_and_evaluate` function uses binary cross-entropy with logits loss (`nn.BCEWithLogitsLoss()`) as the loss function, stochastic gradient descent (`SGD()`) as the optimizer, and uses the PyTorch `flatten()` function to reshape the model's output tensor to match the shape of the labels tensor before passing them to the loss function. The optimizer is applied after computing the gradients with respect to the loss for each batch of the training dataset. Finally, the accuracy is calculated by computing the number of true positive and true negative predictions divided by the total number of predictions for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T08:20:57.049822Z",
     "iopub.status.busy": "2023-04-13T08:20:57.049359Z",
     "iopub.status.idle": "2023-04-13T08:20:57.059047Z",
     "shell.execute_reply": "2023-04-13T08:20:57.058491Z",
     "shell.execute_reply.started": "2023-04-13T08:20:57.049799Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LR = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "    \n",
    "def train_and_evaluate(model, train_data, val_data):\n",
    "    train_labels = train_data['target'].values\n",
    "    train_texts = train_data['text'].values\n",
    "    \n",
    "    val_labels = val_data['target'].values\n",
    "    val_texts = val_data['text'].values\n",
    "\n",
    "    # tokenize texts\n",
    "    train_inputs = tokenizer(list(train_texts), padding=True, truncation=True, max_length=32, return_tensors='pt')\n",
    "    val_inputs = tokenizer(list(val_texts), padding=True, truncation=True, max_length=32, return_tensors='pt')\n",
    "\n",
    "    # create dataset and dataloader\n",
    "    dataset_train = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], torch.tensor(train_labels))\n",
    "    dataset_val = torch.utils.data.TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], torch.tensor(val_labels))\n",
    "    \n",
    "    dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "    dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=32, shuffle=True)\n",
    "\n",
    "    # set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # define loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "\n",
    "    # move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        for input_ids, attention_mask, labels in tqdm(dataloader_train):\n",
    "            # move inputs and labels to device\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.flatten(), labels.float())\n",
    "\n",
    "            # backward pass and optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate metrics\n",
    "            acc = ((outputs > 0) == labels.unsqueeze(-1)).sum().item()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "            \n",
    "        model.eval()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        \n",
    "        for input_ids, attention_mask, labels in tqdm(dataloader_val):\n",
    "            # move inputs and labels to device\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.flatten(), labels.float())\n",
    "            \n",
    "            # calculate metrics\n",
    "            acc = ((outputs > 0) == labels.unsqueeze(-1)).sum().item()\n",
    "            total_loss_val += loss.item()\n",
    "            total_acc_val += acc\n",
    "\n",
    "        train_loss = epoch_loss / len(dataset_train)\n",
    "        train_accuracy = epoch_acc / len(dataset_train)\n",
    "        \n",
    "        val_loss = total_loss_val / len(dataset_val)\n",
    "        val_accuracy = total_acc_val / len(dataset_val)\n",
    "            \n",
    "        \n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{10}: train_loss={train_loss:.4f}, train_accuracy={train_accuracy:.4f}, val_loss={val_loss:.4f}, val_accuracy={val_accuracy:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a `train()` function that takes in a pre-trained `model` and a `train_data` dataset, and fine-tunes the model on the training data.\n",
    "\n",
    "The function first extracts the labels and texts from the training data, tokenizes the texts using the `tokenizer` object, and creates a `TensorDataset` and a `DataLoader` for the training data.\n",
    "\n",
    "Then, the function moves the model to the appropriate device (GPU or CPU), defines the loss function and optimizer (using the same values for `LR` and `MOMENTUM` as in the `train_and_evaluate()` function), and enters a training loop that runs for `EPOCHS` epochs.\n",
    "\n",
    "In each epoch, the function iterates over the batches in the training data, moves the inputs and labels to the device, performs a forward pass through the model to obtain the outputs, calculates the loss and accuracy, performs backpropagation and an optimization step, and accumulates the epoch-level loss and accuracy metrics.\n",
    "\n",
    "At the end of each epoch, the function prints out the epoch-level training loss and accuracy.\n",
    "\n",
    "Finally, the function returns the trained model.\n",
    "\n",
    "Note that this function does not evaluate the trained model on a validation set or perform early stopping, so it is not a complete training and evaluation pipeline. It is intended to be used as a helper function within a larger pipeline that includes validation and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T08:20:57.060177Z",
     "iopub.status.busy": "2023-04-13T08:20:57.059983Z",
     "iopub.status.idle": "2023-04-13T08:20:57.066190Z",
     "shell.execute_reply": "2023-04-13T08:20:57.065674Z",
     "shell.execute_reply.started": "2023-04-13T08:20:57.060158Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_data):\n",
    "    train_labels = train_data['target'].values\n",
    "    train_texts = train_data['text'].values\n",
    "\n",
    "    # tokenize texts\n",
    "    train_inputs = tokenizer(list(train_texts), padding=True, truncation=True, max_length=32, return_tensors='pt')\n",
    "    \n",
    "    # create dataset and dataloader\n",
    "    dataset_train = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], torch.tensor(train_labels))\n",
    "    \n",
    "    dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "\n",
    "    # set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # define loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "\n",
    "    # move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        for input_ids, attention_mask, labels in tqdm(dataloader_train):\n",
    "            # move inputs and labels to device\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.flatten(), labels.float())\n",
    "\n",
    "            # backward pass and optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate metrics\n",
    "            acc = ((outputs > 0) == labels.unsqueeze(-1)).sum().item()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "\n",
    "        train_loss = epoch_loss / len(dataset_train)\n",
    "        train_accuracy = epoch_acc / len(dataset_train)\n",
    "        \n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{10}: train_loss={train_loss:.4f}, train_accuracy={train_accuracy:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating the model\n",
    "\n",
    "The code first creates an instance of the `BertClassifier` class, which is a PyTorch model that uses the BERT architecture for text classification.\n",
    "\n",
    "Then, it splits the `train_df` dataframe into two subsets using `train_test_split()` from scikit-learn, with a train size of 80% and stratified sampling based on the `keyword` column. The resulting subsets are assigned to `df_train` and `df_val`.\n",
    "\n",
    "Finally, the `train_and_evaluate()` function is called with the `bert` model, `df_train` as the training data, and `df_val` as the validation data. The function trains the model on the training data, evaluates it on the validation data, and returns the trained model. The variable `model` is assigned the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T08:20:57.067241Z",
     "iopub.status.busy": "2023-04-13T08:20:57.067042Z",
     "iopub.status.idle": "2023-04-13T08:24:11.114791Z",
     "shell.execute_reply": "2023-04-13T08:24:11.113923Z",
     "shell.execute_reply.started": "2023-04-13T08:20:57.067223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2bad19e0b040e9bcd885341f935fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 191/191 [00:16<00:00, 11.28it/s]\n",
      "100%|██████████| 48/48 [00:01<00:00, 39.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: train_loss=0.0213, train_accuracy=0.5677, val_loss=0.0209, val_accuracy=0.5817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:16<00:00, 11.81it/s]\n",
      "100%|██████████| 48/48 [00:01<00:00, 38.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: train_loss=0.0203, train_accuracy=0.6284, val_loss=0.0193, val_accuracy=0.7367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:16<00:00, 11.69it/s]\n",
      "100%|██████████| 48/48 [00:01<00:00, 38.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: train_loss=0.0182, train_accuracy=0.7415, val_loss=0.0167, val_accuracy=0.7754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:16<00:00, 11.54it/s]\n",
      "100%|██████████| 48/48 [00:01<00:00, 37.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: train_loss=0.0159, train_accuracy=0.7810, val_loss=0.0151, val_accuracy=0.7787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:16<00:00, 11.48it/s]\n",
      "100%|██████████| 48/48 [00:01<00:00, 37.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: train_loss=0.0145, train_accuracy=0.8021, val_loss=0.0138, val_accuracy=0.8109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:16<00:00, 11.44it/s]\n",
      "100%|██████████| 48/48 [00:01<00:00, 37.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: train_loss=0.0138, train_accuracy=0.8107, val_loss=0.0132, val_accuracy=0.8181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:16<00:00, 11.40it/s]\n",
      "100%|██████████| 48/48 [00:01<00:00, 36.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: train_loss=0.0132, train_accuracy=0.8156, val_loss=0.0130, val_accuracy=0.8175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:16<00:00, 11.38it/s]\n",
      "100%|██████████| 48/48 [00:01<00:00, 37.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: train_loss=0.0127, train_accuracy=0.8273, val_loss=0.0131, val_accuracy=0.8207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:16<00:00, 11.37it/s]\n",
      "100%|██████████| 48/48 [00:01<00:00, 36.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: train_loss=0.0125, train_accuracy=0.8314, val_loss=0.0135, val_accuracy=0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:16<00:00, 11.34it/s]\n",
      "100%|██████████| 48/48 [00:01<00:00, 37.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: train_loss=0.0121, train_accuracy=0.8348, val_loss=0.0128, val_accuracy=0.8299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert = BertClassifier()\n",
    "\n",
    "df_train, df_val = train_test_split(train_df, train_size=0.8, stratify=train_df['keyword'])\n",
    "\n",
    "# train bert and ecobert and benchmark the results\n",
    "model = train_and_evaluate(bert, df_train, df_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on the entire training dataset\n",
    "\n",
    "This code is very similar to the previous one, but it trains the BERT classifier on the entire `train_df` dataset, without splitting it into a training and validation set.\n",
    "\n",
    "The `train` function takes in the BERT model and the training data, preprocesses the data, creates a dataset and dataloader, defines the loss function and optimizer, moves the model to the device, and then trains the model for a specified number of epochs.\n",
    "\n",
    "At each epoch, the function loops through the batches in the dataloader, moves the inputs and labels to the device, performs a forward pass to obtain the outputs, computes the loss and performs a backward pass and optimization step to update the model's parameters. It also calculates the accuracy and loss for the entire training set at the end of each epoch.\n",
    "\n",
    "The function returns the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T08:24:11.116474Z",
     "iopub.status.busy": "2023-04-13T08:24:11.116198Z",
     "iopub.status.idle": "2023-04-13T08:27:42.933935Z",
     "shell.execute_reply": "2023-04-13T08:27:42.933117Z",
     "shell.execute_reply.started": "2023-04-13T08:24:11.116453Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 238/238 [00:20<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: train_loss=0.0208, train_accuracy=0.6128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:20<00:00, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: train_loss=0.0175, train_accuracy=0.7423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:21<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: train_loss=0.0148, train_accuracy=0.7936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:20<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: train_loss=0.0137, train_accuracy=0.8089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:21<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: train_loss=0.0132, train_accuracy=0.8157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:21<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: train_loss=0.0128, train_accuracy=0.8256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:21<00:00, 11.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: train_loss=0.0124, train_accuracy=0.8278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:21<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: train_loss=0.0121, train_accuracy=0.8342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:21<00:00, 11.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: train_loss=0.0116, train_accuracy=0.8466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:21<00:00, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: train_loss=0.0115, train_accuracy=0.8472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert = BertClassifier()\n",
    "\n",
    "# train bert and ecobert and benchmark the results\n",
    "model = train(bert, train_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions\n",
    "\n",
    "This code defines a function `predict` that takes a trained model and a text sentence as inputs and predicts the sentiment of the sentence using the model. The sentiment prediction is performed by first tokenizing the sentence using the `tokenizer` function with the same parameters used during training, and then passing the resulting token IDs and attention mask to the model to obtain a predicted score, which is then converted to a probability using the sigmoid function. The predicted probability is then rounded to the nearest integer to obtain the predicted sentiment label.\n",
    "\n",
    "The function is then used to generate predictions for a test dataset by applying it to each row of the `text` column of the `test_df` DataFrame and assigning the predicted labels to the `target` column of a `sample_submission_df` DataFrame. The `progress_apply` function from the `tqdm` package is used to display a progress bar during the prediction process. Note that the trained model must be in evaluation mode by calling the `eval` method before making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T08:27:42.936687Z",
     "iopub.status.busy": "2023-04-13T08:27:42.936468Z",
     "iopub.status.idle": "2023-04-13T08:28:04.155218Z",
     "shell.execute_reply": "2023-04-13T08:28:04.154463Z",
     "shell.execute_reply.started": "2023-04-13T08:27:42.936680Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3263/3263 [00:21<00:00, 153.85it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict(model, sentence):\n",
    "    # tokenize the sentence and convert to tensor\n",
    "    inputs = tokenizer(sentence, padding='max_length', max_length=32, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # get the predicted logits for the input\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs['input_ids'].to(device), attention_mask=inputs['attention_mask'].to(device)).flatten()\n",
    "\n",
    "    # convert logits to probabilities and return the predicted label\n",
    "    probs = torch.sigmoid(logits).squeeze()\n",
    "    label = torch.round(probs).item()\n",
    "\n",
    "    return int(label)\n",
    "    \n",
    "model.eval()\n",
    "sample_submission_df['target'] = test_df['text'].progress_apply(lambda x: predict(model, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T08:28:04.156224Z",
     "iopub.status.busy": "2023-04-13T08:28:04.156025Z",
     "iopub.status.idle": "2023-04-13T08:28:04.167632Z",
     "shell.execute_reply": "2023-04-13T08:28:04.167008Z",
     "shell.execute_reply.started": "2023-04-13T08:28:04.156205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3263.000000</td>\n",
       "      <td>3263.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5427.152927</td>\n",
       "      <td>0.368986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3146.427221</td>\n",
       "      <td>0.482604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2683.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8176.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10875.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id       target\n",
       "count   3263.000000  3263.000000\n",
       "mean    5427.152927     0.368986\n",
       "std     3146.427221     0.482604\n",
       "min        0.000000     0.000000\n",
       "25%     2683.000000     0.000000\n",
       "50%     5500.000000     0.000000\n",
       "75%     8176.000000     1.000000\n",
       "max    10875.000000     1.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T08:28:04.168838Z",
     "iopub.status.busy": "2023-04-13T08:28:04.168415Z",
     "iopub.status.idle": "2023-04-13T08:28:04.177661Z",
     "shell.execute_reply": "2023-04-13T08:28:04.177014Z",
     "shell.execute_reply.started": "2023-04-13T08:28:04.168817Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "85351c312530d573d7e44902c9cd1a55dfa0fa69584cb9cbd08e6a51fb86ad5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
